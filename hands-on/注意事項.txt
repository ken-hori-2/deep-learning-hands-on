

注意事項:
    pineappleだと、appleという文字が含まれるので、appleとしてカウントされてしまうので注意


メモ:
    ミニバッチ学習:
        "過学習を防ぐ(局所的にならないようにする)" + "学習の安定化(変な方向に学習が偏らないように)" のための手法
        なので、学習時は必要だが、評価時はなくてもいい
        ミニバッチ学習を行わない = データを分けないので、1epochにつきすべてのデータを1回で学習してしまう
    
    trainモードとevalモード:
        おそらく同じ関数内で実装する場合に必要
        # 訓練モード: normalize, dropout = on
        # 評価モード: normalize, dropout = off
        別々の関数で実行する際はなくてもいいかも？(確認中)
    

    データ拡張:
        バッチを取り出す際に元の画像に対してランダムに変換される
        学習の効率化: 入力の平均値を0, 標準偏差を1にする(標準化)
    

    CNNモデルの入力と出力サイズ:
        Output_h(縦) = {(Input_h - Filter_h + 2*padding_num)/stride_num} + 1
        Output_w(幅) = {(Input_w - Filter_w + 2*padding_num)/stride_num} + 1

        Pooling:
            入力サイズをプーリングサイズで割ったものが出力
            Output size = (Input size) / (Pooling size)

        例1:
            Input size=32*32, Filter=5, padding=None, stride=1の場合:
                Output = {(32 - 5 + 2*0)/1} + 1
                    = 27 + 1
                    = 28 ... 28*28
        
        例2: # 見やすいように一部変更　モデルの各Input, Outputを追加
                            # 畳み込み層:(入力チャンネル数, フィルタ数、フィルタサイズ)
            1. self.conv1 = nn.Conv2d(3, 6, 5) # In=32*32 -> Out=28*28
                            # 活性化関数
            -. self.relu = nn.ReLU()
                            # プーリング層:（領域のサイズ, ストライド）
            2. self.pool = nn.MaxPool2d(2, 2)  # In=28*28 -> Out=14*14
                            # 畳み込み層:(入力チャンネル数, フィルタ数、フィルタサイズ)
            3. self.conv2 = nn.Conv2d(6, 16, 5) # In=14*14 -> Out=10*10
                            # 活性化関数
            -. self.relu = nn.ReLU()
                            # プーリング層:（領域のサイズ, ストライド）
            -. self.pool = nn.MaxPool2d(2, 2) # In=10*10 -> Out=5*5
                            # 全結合層
                            > channel数はその前のフィルター数と同じ
            4. self.fc1 = nn.Linear(16*5*5, 256) # In=size(5*5)*channel(16) -> Out=256 # 1次元配列で入出力
                            # ドロップアウト:(p=ドロップアウト率)
            5. self.dropout = nn.Dropout(p=0.5) # 0.5の確率でランダムにニューロンを無効化
                            # 全結合層
            6. self.fc2 = nn.Linear(256, 10)