import torch
import torch.nn as nn

# ハイパーパラメータの設定
vocab_size = 10  # 語彙サイズ
embed_dim = 50   # 埋め込みベクトルの次元数

# nn.Embeddingのインスタンスを作成
embedding_layer = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)

# 入力データ（単語IDのテンソル）
input_data = torch.tensor([0, 1, 2, 3, 4], dtype=torch.long)  # 単語IDの例

# 埋め込み層を通じて入力データを変換
embedded_data = embedding_layer(input_data)

# 埋め込まれたデータの表示
print(embedded_data)

# 出力例

# tensor([[ 1.2979e-01, -6.9964e-01,  5.0297e-01,  1.4355e+00,  6.8153e-01,
#           3.5840e-02, -3.9593e-01,  1.0861e+00,  8.9398e-01, -6.7448e-01,
#           3.4145e-01, -1.4084e+00, -4.1465e-01,  4.9397e-01, -1.9115e+00,
#          -4.7952e-01,  5.7964e-01,  1.6009e+00,  1.9510e+00,  4.3784e-01,
#          -4.1091e-01,  1.6697e+00,  7.3877e-01, -1.0039e-01, -1.9795e-02,
#          -1.2547e+00,  6.8411e-01,  3.9364e-01,  1.5668e-01,  1.7594e+00,
#          -1.0131e+00, -3.6218e-01,  1.7136e+00,  1.3347e+00, -9.6188e-01,
#          -3.0395e-02,  1.4834e+00, -1.1741e+00, -1.2802e+00, -1.3956e+00,
#          -7.6017e-01, -9.0091e-01,  6.9307e-01, -5.3103e-01,  4.7999e-01,
#           1.8059e-01,  4.9357e-01,  1.6881e+00,  9.1066e-01, -2.9888e-01], : 50個

#           ×5個分

# == tensor([[x11, x12, x13, ..., x1 50],
#           [x21, x22, x23, ..., x2 50],
#           ...
#           ])